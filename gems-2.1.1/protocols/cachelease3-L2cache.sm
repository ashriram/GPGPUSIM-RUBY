
/*
    Copyright (C) 1999-2005 by Mark D. Hill and David A. Wood for the
    Wisconsin Multifacet Project.  Contact: gems@cs.wisc.edu
    http://www.cs.wisc.edu/gems/

    --------------------------------------------------------------------

    This file is part of the SLICC (Specification Language for
    Implementing Cache Coherence), a component of the Multifacet GEMS
    (General Execution-driven Multiprocessor Simulator) software
    toolset originally developed at the University of Wisconsin-Madison.

    SLICC was originally developed by Milo Martin with substantial
    contributions from Daniel Sorin.

    Substantial further development of Multifacet GEMS at the
    University of Wisconsin was performed by Alaa Alameldeen, Brad
    Beckmann, Jayaram Bobba, Ross Dickson, Dan Gibson, Pacia Harper,
    Derek Hower, Milo Martin, Michael Marty, Carl Mauer, Michelle Moravan,
    Kevin Moore, Manoj Plakal, Daniel Sorin, Haris Volos, Min Xu, and Luke Yen.

    --------------------------------------------------------------------

    If your use of this software contributes to a published paper, we
    request that you (1) cite our summary paper that appears on our
    website (http://www.cs.wisc.edu/gems/) and (2) e-mail a citation
    for your published paper to gems@cs.wisc.edu.

    If you redistribute derivatives of this software, we request that
    you notify us and either (1) ask people to register with us at our
    website (http://www.cs.wisc.edu/gems/) or (2) collect registration
    information and periodically send it to us.

    --------------------------------------------------------------------

    Multifacet GEMS is free software; you can redistribute it and/or
    modify it under the terms of version 2 of the GNU General Public
    License as published by the Free Software Foundation.

    Multifacet GEMS is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
    General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with the Multifacet GEMS; if not, write to the Free Software
    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
    02111-1307, USA

    The GNU General Public License is contained in the file LICENSE.

### END HEADER ###
 */
/*
 * $Id: MSI_MOSI_CMP_directory-L2cache.sm 1.12 05/01/19 15:55:40-06:00 beckmann@s0-28.cs.wisc.edu $
 *
 */

machine(L2Cache, "Cache Leasing v3 L2 Cache CMP") {

    // L2 BANK QUEUES
    // From local bank of L2 cache TO the network
    MessageBuffer responseFromL2Cache, network="To", virtual_network="1", ordered="true";  // this L2 bank -> a local L1

    // FROM the network to this local bank of L2 cache
    MessageBuffer L1RequestToL2Cache, network="From", virtual_network="0", ordered="true";  // a local L1 -> this L2 bank

    // STATES
    enumeration(State, desc="L2 Cache states", default="L2Cache_State_NP") {
        // Base states
        NP, desc="Not present in cache";
        E, desc="L2 cache entry valid but with expired timestamp";
        S, desc="L2 cache entry valid with single sharer";
        SS, desc="L2 cache entry valid with possibly multiple sharers, but at least one";

        // Transient States for fetching data from memory
        IS, desc="L2 idle, got L1_GETS, issued memory fetch, have not seen response yet";
        IM, desc="L2 idle, got L1 Write, issued memory fetch, have not seen response(s) yet";
        IMA, desc="L2 idle, got Atomic, issued memory fetch, have not seen response(s) yet";
        MI, desc="L2 replacement on valid timestamp, waiting for expire event";
    }

    // EVENTS
    enumeration(Event, desc="L2 Cache events") {
        // L2 events

        // events initiated by the local L1s
        L1_GETS,                 desc="a L1D GETS request for a block maped to us";

        // Store from L1
        L1_Upgrade_T,  desc="data from L1 due to a write hit in L1. Timestamps between L1 and L2 match.";
        L1_Upgrade_NT,  desc="data from L1 due to a write hit in L1. Timestamps between L1 and L2 do not match.";
        L1_Write,  desc="data from L1 due to a write miss in L1";

        // Atomic from L1
        L2_Atomic,  desc="Atomic request and data from L1";

        // Expiring event
        L2_Expire,  desc="L2 block lifetime expired";

        // events initiated by this L2
        L2_Replacement,     desc="L2 Replacement", format="!r";
        L2_Replacement_clean,     desc="L2 Replacement, but data is clean", format="!r";

        // events from memory controller
        Mem_Data,     desc="data from memory, single reader waiting", format="!r";
        Mem_Data_SS,     desc="data from memory, multiple readers waiting", format="!r";
    }

    // TYPES

    // Interface to off-chip GDDR memory
        external_type(GpusimDramInterface, inport="yes", outport="yes") {
    }

    // CacheEntry
    structure(Entry, desc="...", interface="AbstractCacheEntry") {
        State CacheState,          desc="cache state";
        DataBlock DataBlk,       desc="data for the block";
        bool Dirty, default="false", desc="data is dirty";
        Time liveUntilTime,     desc="maximum lifetime of the cache block in any of the L1s";

        // For PCAdaptive predictor
        IntPairSet SharerPCPairSet,       desc="Sharer,PC pairs that have read from this cache block";
    }

    // TBE fields
    structure(TBE, desc="...") {
        Address Address,            desc="Physical address for this TBE";
        State TBEState,             desc="Transient state";
        DataBlock DataBlk,          desc="Buffer for the data block";
        bool Dirty, default="false", desc="Data is Dirty";

        NetDest L1_GetS_IDs,            desc="Set of the internal processors that want the block in shared state";
        MachineID L1_WBAck_ID,          desc="ID of the L1 cache to forward the block to once we get a response";

        bool isPrefetch,            desc="Set if this was caused by a prefetch";
        MemorySpaceType Space,        desc="Memory space of request (GLOBAL, LOCAL)";
        Time deltaRequested,        desc="Delta time requested";

        int RequestSize,            desc="Size of request data";
        Time liveUntilTime,         desc="Live until time for L2 replacement TBEs";
        uint64 memfetch,               desc="Memfetch of requestor thread";
        // only loads use mfset because store requests can never be merged
        MemfetchSet mfset,            desc="Set of memfetchs received so far in this TBE";
    }

    external_type(CacheMemory) {
        bool cacheAvail(Address);
        Address cacheProbe(Address);
        void allocate(Address);
        void deallocate(Address);
        Entry lookup(Address);
        void changePermission(Address, AccessPermission);
        bool isTagPresent(Address);
        void setMRU(Address);
    }

    external_type(TBETable) {
        TBE lookup(Address);
        void allocate(Address);
        void deallocate(Address);
        bool isPresent(Address);
    }

    // Objects
    GpusimDramInterface GpusimDramBuffer, abstract_chip_ptr="true", constructor_hack="i";

    TBETable L2_TBEs, template_hack="<L2Cache_TBE>";

    CacheMemory L2cacheMemory, template_hack="<L2Cache_Entry>", constructor_hack='L2_CACHE_NUM_SETS_BITS,L2_CACHE_ASSOC,MachineType_L2Cache,int_to_string(i)';

    MessageBuffer L2ExpireTriggerQueue,  ordered="false";

    // inclusive cache, returns L2 entries only
    Entry getL2CacheEntry(Address addr), return_by_ref="yes" {
            return L2cacheMemory[addr];
    }

    void changeL2Permission(Address addr, AccessPermission permission) {
        if (L2cacheMemory.isTagPresent(addr)) {
            return L2cacheMemory.changePermission(addr, permission);
        }
    }

    string getCoherenceRequestTypeStr(CoherenceRequestType type) {
        return CoherenceRequestType_to_string(type);
    }

    bool isL2CacheTagPresent(Address addr) {
        return (L2cacheMemory.isTagPresent(addr));
    }

    bool isLive(Address addr) {
        assert(isL2CacheTagPresent(addr));
        return time_to_int(getL2CacheEntry(addr).liveUntilTime) >= time_to_int(get_time());
    }


    State getState(Address addr) {
        if(L2_TBEs.isPresent(addr)) {
            return L2_TBEs[addr].TBEState;
        } else if (isL2CacheTagPresent(addr)) {
            if (isLive(addr) == false && (getL2CacheEntry(addr).CacheState == State:SS || getL2CacheEntry(addr).CacheState == State:S)) {
               return State:E;
            } else {
               return getL2CacheEntry(addr).CacheState;
            }
        }
        return State:NP;
    }

    string getStateStr(Address addr) {
        return L2Cache_State_to_string(getState(addr));
    }

    // when is this called
    void setState(Address addr, State state) {

        // MUST CHANGE
        if (L2_TBEs.isPresent(addr)) {
            L2_TBEs[addr].TBEState := state;
        }

        if (isL2CacheTagPresent(addr)) {
            getL2CacheEntry(addr).CacheState := state;

            // Set permission
            if (state == State:SS || state == State:S || state == State:E) {
                changeL2Permission(addr, AccessPermission:Read_Write);
            } else {
                changeL2Permission(addr, AccessPermission:Busy);
            }
        }
    }

    Event L1Cache_request_type_to_event(CoherenceRequestType type, Address addr, Time msgLiveUntilTime) {
        if(type == CoherenceRequestType:GETS) {
            return Event:L1_GETS;
        } else if(type == CoherenceRequestType:DATA) {
            return Event:L1_Write;
        } else if (type == CoherenceRequestType:DATA_ATOMIC) {
           return Event:L2_Atomic;
        } else if (type == CoherenceRequestType:DATA_UPGRADE) {
            // L1 Upgrade event
            // If L2 block is allocated and still not expired, then check for timestamp match - if so trigger L1_Upgrade_T
            // Otherwise trigger L1_Upgrade_NT
            if (isL2CacheTagPresent(addr) && isLive(addr)==true) {
                if(getL2CacheEntry(addr).liveUntilTime == msgLiveUntilTime) {
                    // Timestamps match
                    return Event:L1_Upgrade_T;
                } else {
                    // Timestamps don't match
                    return Event:L1_Upgrade_NT;
                }
            } else {
                // Not in cache or timestamp expired
                return Event:L1_Upgrade_NT;
            }
        } else {
            DEBUG_EXPR(addr);
            DEBUG_EXPR(type);
            error("Invalid L1 forwarded request type");
        }
    }

    void addSharer(Address addr, MachineID requestor) {
        assert(false);
    }

    bool isOneSharerLeft(Address addr, MachineID requestor) {
        assert(false);
        return false;
    }

    bool isSharer(Address addr, MachineID requestor) {
        assert(false);
        return false;
    }

    Event L1request_to_event(CoherenceRequestType type, Address addr, Time msgLiveUntilTime) {
         if(type == CoherenceRequestType:DATA) {
             return Event:L1_Write;
         } else if (type == CoherenceRequestType:DATA_ATOMIC) {
            return Event:L2_Atomic;
         } else if (type == CoherenceRequestType:DATA_UPGRADE) {
             // L1 Upgrade event
             // If L2 block is allocated and still not expired, then check for timestamp match - if so trigger L1_Upgrade_T
             // Otherwise trigger L1_Upgrade_NT
             if (isL2CacheTagPresent(addr) && isLive(addr)==true) {
                 if(getL2CacheEntry(addr).liveUntilTime == msgLiveUntilTime) {
                     // Timestamps match
                     return Event:L1_Upgrade_T;
                 } else {
                     // Timestamps don't match
                     return Event:L1_Upgrade_NT;
                 }
             } else {
                 // Not in cache or timestamp expired
                 return Event:L1_Upgrade_NT;
             }
         } else {
             error("unknown message type");
         }
    }

    // Get L2 block's new global time after read request is serviced
    Time globalTimeAfterRead(Time requested, Time currentLiveUntilTime) {
        return getTimeMax(getTimePlusTime(get_time(),requested), currentLiveUntilTime);
    }

    // Get L2 block's new global time after write request is serviced
    Time globalTimeAfterWrite(Time requested, Time currentLiveUntilTime) {
        // Take the max of (now+requested, liveUntil+1).
        // Writes request 0 time, so they will increment. Upgrades may extend time.
        return getTimeMax( getTimePlusTime(get_time(),requested) , getTimePlusInt(currentLiveUntilTime,1) );
    }

    // ** OUT_PORTS **

    out_port(memQueue_out, MemoryMsg, GpusimDramBuffer);
    out_port(responseIntraChipL2Network_out, ResponseMsg, responseFromL2Cache);

    out_port(L2ExpireTriggerQueue_out, L2ExpireTriggerMsg, L2ExpireTriggerQueue);

    // ** IN_PORTS **

    // L2 Expire Trigger Queue
    in_port(L2ExpireTriggerQueue_in, L2ExpireTriggerMsg, L2ExpireTriggerQueue) {
        if (L2ExpireTriggerQueue_in.isReady()) {
            peek(L2ExpireTriggerQueue_in, L2ExpireTriggerMsg) {
                assert(time_to_int(in_msg.ExpireTime) <= time_to_int(get_time()));    // 'Less than' allowed because we might be retiring only one expire event per cycle
                // Only trigger event if state is MI and the expire time matches that in TBE, otherwise pop this message
                if(getState(in_msg.Address)==State:MI && time_to_int(L2_TBEs[in_msg.Address].liveUntilTime) < time_to_int(in_msg.ExpireTime)) {
                    trigger(Event:L2_Expire, in_msg.Address);
                } else {
                    L2ExpireTriggerQueue_in.dequeue();
                }
            }
        }
    }


    // off-chip memory request/response is done
    in_port(memQueue_in, MemoryMsg, GpusimDramBuffer) {
      if (memQueue_in.isReady()) {
        peek(memQueue_in, MemoryMsg) {
          if (in_msg.Type == MemoryRequestType:MEMORY_READ) {
              if(L2_TBEs[in_msg.Address].L1_GetS_IDs.count() > 1) {
                  trigger(Event:Mem_Data_SS, in_msg.Address);  // Multiple readers waiting
              } else {
                  trigger(Event:Mem_Data, in_msg.Address);  // Single reader waiting
              }
          } else if (in_msg.Type == MemoryRequestType:MEMORY_WB) {
            // Pop DRAM Ack without event
            memQueue_in.dequeue();
          } else {
            DEBUG_EXPR(in_msg.Type);
            error("Invalid message");
          }
        }
      }
    }

    // L1 Request
    in_port(L1RequestIntraChipL2Network_in, RequestMsg, L1RequestToL2Cache) {
        if(L1RequestIntraChipL2Network_in.isReady()) {
            peek(L1RequestIntraChipL2Network_in,  RequestMsg) {

                assert(machineIDToMachineType(in_msg.Requestor) == MachineType:L1Cache);
                assert(in_msg.Destination.isElement(machineID));

                if (L2cacheMemory.isTagPresent(in_msg.Address)) {
                    // The L2 contains the block, so proceeded with handling the request
                    trigger(L1Cache_request_type_to_event(in_msg.Type, in_msg.Address, in_msg.liveUntilTime), in_msg.Address);
                } else {
                    if (L2cacheMemory.cacheAvail(in_msg.Address)) {
                        // L2 does't have the line, but we have space for it in the L2
                        trigger(L1Cache_request_type_to_event(in_msg.Type, in_msg.Address, in_msg.liveUntilTime), in_msg.Address);
                    } else {
                        // No room in the L2, so we need to make room before handling the request
                        if (L2cacheMemory[ L2cacheMemory.cacheProbe(in_msg.Address) ].Dirty ) {
                            trigger(Event:L2_Replacement, L2cacheMemory.cacheProbe(in_msg.Address));
                        } else {
                            trigger(Event:L2_Replacement_clean, L2cacheMemory.cacheProbe(in_msg.Address));
                        }
                    }
                }

            }
        }
    }


    // ACTIONS

    action(a_issueFetchToMemory, "a", desc="fetch data from memory") {
      peek(L1RequestIntraChipL2Network_in, RequestMsg) {
        enqueue(memQueue_out, MemoryMsg, latency="L2_to_MEM_MSG_LATENCY") {
          out_msg.Address := address;
          out_msg.Type := MemoryRequestType:MEMORY_READ;
          out_msg.Sender := machineID;
          out_msg.OriginalRequestorMachId := machineID;
          out_msg.MessageSize := MessageSizeType:Control;
        }
      }
    }

    action(a_issueFetchToMemoryForStore, "as", desc="fetch data from memory for store") {
      peek(L1RequestIntraChipL2Network_in, RequestMsg) {
        enqueue(memQueue_out, MemoryMsg, latency="L2_to_MEM_MSG_LATENCY") {
          out_msg.Address := address;
          out_msg.Type := MemoryRequestType:MEMORY_READ;
          out_msg.Sender := machineID;
          out_msg.OriginalRequestorMachId := machineID;
          out_msg.MessageSize := MessageSizeType:Control;
        }
      }
    }

    action(c_exclusiveReplacement, "c", desc="Send data to memory") {
      enqueue(memQueue_out, MemoryMsg, latency="L2_to_MEM_DATA_LATENCY") {
        out_msg.Address := address;
        out_msg.Type := MemoryRequestType:MEMORY_WB;
        out_msg.Sender := machineID;
        out_msg.OriginalRequestorMachId := machineID;
        out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
      }
    }

    action(ds_sendDataToRequestor, "ds", desc="Send data+timestamp from cache to requestor in request queue") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            assert(time_to_int(getL2CacheEntry(address).liveUntilTime) >= time_to_int(get_time()));
            enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_to_L1_DATA_LATENCY") {
                out_msg.Address := address;
                out_msg.Type := CoherenceResponseType:DATA;
                out_msg.Sender := machineID;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
                out_msg.Dirty := getL2CacheEntry(address).Dirty;
                out_msg.MessageSize := IntToMessageSizeType(in_msg.RequestSize);
                out_msg.AckCount := 0;
                out_msg.Space := in_msg.Space;
                out_msg.RequestSize := in_msg.RequestSize;
                out_msg.liveUntilTime := getL2CacheEntry(address).liveUntilTime;
                // response to read, no GWCT here
                out_msg.mfset.insert(in_msg.Requestor, in_msg.memfetch);
            }
        }
    }

    action(dst_sendDataToGetSRequestors, "dst", desc="Send data+timestamp from cache to all GetS IDs from TBE") {
        assert(L2_TBEs[address].L1_GetS_IDs.count() > 0);
        assert(time_to_int(getL2CacheEntry(address).liveUntilTime) >= time_to_int(get_time()));
        enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_to_L1_DATA_LATENCY") {
            out_msg.Address := address;
            out_msg.Type := CoherenceResponseType:DATA;
            out_msg.Sender := machineID;
            out_msg.Destination := L2_TBEs[address].L1_GetS_IDs;  // internal nodes
            out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
            out_msg.Dirty := getL2CacheEntry(address).Dirty;
            out_msg.MessageSize := IntToMessageSizeType(L2_TBEs[address].RequestSize);
            out_msg.Space := L2_TBEs[address].Space;
            out_msg.RequestSize := L2_TBEs[address].RequestSize;
            assert(isL2CacheTagPresent(address));        // Note: Sending liveUntilTime from cache, so it must exist!
            out_msg.liveUntilTime := getL2CacheEntry(address).liveUntilTime;
            out_msg.GWCT := getL2CacheEntry(address).liveUntilTime;
            out_msg.mfset := L2_TBEs[address].mfset;
        }
    }

    action(ds_g_sendDataGlobalToRequestor, "ds_g", desc="Send data global from cache to requestor in response queue") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            assert(time_to_int(getL2CacheEntry(address).liveUntilTime) >= time_to_int(get_time()));
            enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_to_L1_DATA_LATENCY") {
                out_msg.Address := address;
                out_msg.Type := CoherenceResponseType:DATA_GLOBAL;
                out_msg.Sender := machineID;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
                out_msg.Dirty := getL2CacheEntry(address).Dirty;
                out_msg.MessageSize := IntToMessageSizeType(in_msg.RequestSize);
                out_msg.AckCount := 0;
                out_msg.Space := in_msg.Space;
                out_msg.RequestSize := in_msg.RequestSize;
                out_msg.liveUntilTime := globalTimeAfterWrite(in_msg.deltaRequested,getL2CacheEntry(address).liveUntilTime); // use new liveUntilTime as it may be extended (upgrades)
                out_msg.GWCT := getL2CacheEntry(address).liveUntilTime;
                out_msg.memfetch := in_msg.memfetch;
            }
        }
    }

    action(ds_sendAtomicDataGlobalToRequestor, "ds_a", desc="Send atomic data from cache to requestor in response queue") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_to_L1_DATA_LATENCY") {
                out_msg.Address := address;
                out_msg.Type := CoherenceResponseType:DATA_ATOMIC;
                out_msg.Sender := machineID;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
                out_msg.Dirty := getL2CacheEntry(address).Dirty;
                out_msg.MessageSize := IntToMessageSizeType(in_msg.RequestSize);
                out_msg.AckCount := 0;
                out_msg.Space := in_msg.Space;
                out_msg.RequestSize := in_msg.RequestSize;
                out_msg.liveUntilTime := getL2CacheEntry(address).liveUntilTime;
                out_msg.GWCT := getL2CacheEntry(address).liveUntilTime;
                out_msg.memfetch := in_msg.memfetch;
            }
        }
    }

    action(dst_sendAtomicDataToGetSRequestors, "dst_a", desc="Send atomic data + timestamp from cache to requestor from TBE") {
        enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_to_L1_DATA_LATENCY") {
            out_msg.Address := address;
            out_msg.Type := CoherenceResponseType:DATA_ATOMIC;
            out_msg.Sender := machineID;
            out_msg.Destination.add(L2_TBEs[address].L1_WBAck_ID);
            out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
            out_msg.Dirty := getL2CacheEntry(address).Dirty;
            out_msg.MessageSize := IntToMessageSizeType(L2_TBEs[address].RequestSize);
            out_msg.Space := L2_TBEs[address].Space;
            out_msg.RequestSize := L2_TBEs[address].RequestSize;
            assert(isL2CacheTagPresent(address));        // Note: Sending liveUntilTime from cache, so it must exist!
            out_msg.liveUntilTime := getL2CacheEntry(address).liveUntilTime;
            out_msg.GWCT := getL2CacheEntry(address).liveUntilTime;
            out_msg.memfetch := L2_TBEs[address].memfetch;
        }
    }

    action(wa_g_sendAckGlobalToRequestor, "wa_g", desc="Send Ack Global from cache to requestor") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_to_L1_MSG_LATENCY") {
                out_msg.Address := address;
                out_msg.Type := CoherenceResponseType:ACK_GLOBAL;
                out_msg.Sender := machineID;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.MessageSize := MessageSizeType:Response_Control;
                out_msg.Space := in_msg.Space;
                out_msg.RequestSize := in_msg.RequestSize;
                out_msg.liveUntilTime := globalTimeAfterWrite(in_msg.deltaRequested,getL2CacheEntry(address).liveUntilTime); // use new liveUntilTime as it may be extended (upgrades)
                out_msg.GWCT := getL2CacheEntry(address).liveUntilTime;
                out_msg.memfetch := in_msg.memfetch;
            }
        }
    }

    action(wagt_sendAckGlobalToRequestorFromTBE, "wagt", desc="Send ack global from cache to requestor from TBE") {
        enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_to_L1_MSG_LATENCY") {
            out_msg.Address := address;
            out_msg.Type := CoherenceResponseType:ACK_GLOBAL;
            out_msg.Sender := machineID;
            out_msg.Destination.add(L2_TBEs[address].L1_WBAck_ID);
            out_msg.MessageSize := MessageSizeType:Response_Control;
            out_msg.Space := L2_TBEs[address].Space;
            out_msg.RequestSize := L2_TBEs[address].RequestSize;
            assert(isL2CacheTagPresent(address));        // Note: Sending liveUntilTime from cache, so it must exist!
            out_msg.liveUntilTime := getL2CacheEntry(address).liveUntilTime;
            out_msg.GWCT := getL2CacheEntry(address).liveUntilTime;
            out_msg.memfetch := L2_TBEs[address].memfetch;
        }
    }

    action(wa_sendAckToRequestor, "wa", desc="Send Ack (non-global) from cache to requestor") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_to_L1_MSG_LATENCY") {
                out_msg.Address := address;
                out_msg.Type := CoherenceResponseType:ACK;
                out_msg.Sender := machineID;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.MessageSize := MessageSizeType:Response_Control;
                out_msg.Space := in_msg.Space;
                out_msg.RequestSize := in_msg.RequestSize;
                out_msg.liveUntilTime := globalTimeAfterWrite(in_msg.deltaRequested,getL2CacheEntry(address).liveUntilTime); // use new liveUntilTime as it may be extended (upgrades)
                out_msg.GWCT := getL2CacheEntry(address).liveUntilTime;
                out_msg.memfetch := in_msg.memfetch;
            }
        }
    }

    // OTHER ACTIONS
    action(i_allocateTBE, "i", desc="Allocate TBE from L1 request queue") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            check_allocate(L2_TBEs);
            L2_TBEs.allocate(address);
            L2_TBEs[address].L1_GetS_IDs.clear();
            L2_TBEs[address].DataBlk := getL2CacheEntry(address).DataBlk;
            L2_TBEs[address].Dirty := getL2CacheEntry(address).Dirty;
            L2_TBEs[address].Space := in_msg.Space;
            L2_TBEs[address].RequestSize := in_msg.RequestSize;
            L2_TBEs[address].deltaRequested := in_msg.deltaRequested;
            L2_TBEs[address].liveUntilTime := int_to_time(0);
            L2_TBEs[address].memfetch := in_msg.memfetch;
        }
    }

    action(is_setTBE, "is", desc="Set TBE from L1 request queue for TBE already allocated by MI") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            assert(L2_TBEs.isPresent(in_msg.Address));
            L2_TBEs[address].L1_GetS_IDs.clear();
            L2_TBEs[address].DataBlk := getL2CacheEntry(address).DataBlk;
            L2_TBEs[address].Dirty := getL2CacheEntry(address).Dirty;
            L2_TBEs[address].Space := in_msg.Space;
            L2_TBEs[address].RequestSize := in_msg.RequestSize;
            L2_TBEs[address].deltaRequested := in_msg.deltaRequested;
            // Don't set liveUntilTime, it should have been set at replacement
            L2_TBEs[address].memfetch := in_msg.memfetch;
        }
    }

    action(il_allocateTBE, "il", desc="Allocate TBE from L1 response queue") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            check_allocate(L2_TBEs);
            L2_TBEs.allocate(address);
            L2_TBEs[address].L1_GetS_IDs.clear();
            L2_TBEs[address].DataBlk := getL2CacheEntry(address).DataBlk;
            L2_TBEs[address].Dirty := getL2CacheEntry(address).Dirty;
            L2_TBEs[address].RequestSize := in_msg.RequestSize;
            L2_TBEs[address].deltaRequested := int_to_time(0);
            L2_TBEs[address].liveUntilTime := int_to_time(0);
            L2_TBEs[address].Space := in_msg.Space;
            L2_TBEs[address].memfetch := in_msg.memfetch;
        }
    }

    action(ils_setTBE, "ils", desc="Set TBE from L1 response queue for TBE already allocated by MI") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            assert(L2_TBEs.isPresent(in_msg.Address));
            L2_TBEs[address].L1_GetS_IDs.clear();
            L2_TBEs[address].DataBlk := getL2CacheEntry(address).DataBlk;
            L2_TBEs[address].Dirty := getL2CacheEntry(address).Dirty;
            L2_TBEs[address].RequestSize := in_msg.RequestSize;
            L2_TBEs[address].deltaRequested := int_to_time(0);
            // Don't set liveUntilTime, it should have been set at replacement
            L2_TBEs[address].Space := in_msg.Space;
            L2_TBEs[address].memfetch := in_msg.memfetch;
        }
    }

    action(ir_allocateTBE, "ir", desc="Allocate TBE for L2 replacement") {
        check_allocate(L2_TBEs);
        L2_TBEs.allocate(address);
        L2_TBEs[address].L1_GetS_IDs.clear();
        L2_TBEs[address].DataBlk := getL2CacheEntry(address).DataBlk;
        L2_TBEs[address].Dirty := getL2CacheEntry(address).Dirty;
        L2_TBEs[address].liveUntilTime := getL2CacheEntry(address).liveUntilTime;
    }

    action(it_insertL2ExpireTriggerQueue, "it", desc="Insert expire event into L2 expire trigger queue") {
        // Latency hack for L2 Expire trigger queue
        enqueue(L2ExpireTriggerQueue_out, L2ExpireTriggerMsg, latency="(out_msg).m_ExpireTime-get_time()") {
            out_msg.Address := address;
            // The expire time should be lifeUntilTime+1
            out_msg.ExpireTime := getTimePlusInt(getL2CacheEntry(address).liveUntilTime,1);
            assert( time_to_int(out_msg.ExpireTime) > time_to_int(get_time()) );
        }
        // Profile stall caused by unexpired evicts
        profileCLStallEvict(getTimeMinusTime( getTimePlusInt(getL2CacheEntry(address).liveUntilTime,1) ,  get_time()  ));
    }

    action(s_deallocateTBE, "s", desc="Deallocate external TBE") {
        L2_TBEs.deallocate(address);
    }

    action(jj_popL1RequestQueue, "\j", desc="Pop incoming L1 request queue") {
        peek(L1RequestIntraChipL2Network_in,  RequestMsg) {
           if(in_msg.Type == CoherenceRequestType:GETS) {
              profileBandwidth("L1_MSG", in_msg.MessageSize);
           } else if (in_msg.Type == CoherenceRequestType:DATA ||
                      in_msg.Type == CoherenceRequestType:DATA_UPGRADE) {
              profileBandwidth("L1_DATA", in_msg.MessageSize);
           } else if (in_msg.Type == CoherenceRequestType:DATA_ATOMIC) {
              profileBandwidth("L1_ATOMIC", in_msg.MessageSize);
           } else {
              error("Invalid CacheResponseType");
           }
        }
        L1RequestIntraChipL2Network_in.dequeue();
    }


    action(o_popIncomingResponseQueue, "o", desc="Pop Incoming Response queue") {
        memQueue_in.dequeue();
    }

    action(ot_popL2ExpireTriggerQueue, "ot", desc="Pop L2 Expire Trigger Queue") {
        L2ExpireTriggerQueue_in.dequeue();
    }


    action(m_writeDataToCache, "m", desc="Write data from mem response queue to cache") {
      peek(memQueue_in, MemoryMsg) {
        getL2CacheEntry(address).DataBlk := in_msg.DataBlk;
        getL2CacheEntry(address).Dirty := false;
      }
    }

    action(mr_writeDataToCacheFromRequest, "mr", desc="Write data from L1 response queue to cache") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            assert(in_msg.Dirty == true);
            getL2CacheEntry(address).DataBlk := in_msg.DataBlk;
            getL2CacheEntry(address).Dirty := in_msg.Dirty;
        }
    }

    action(mt_writeDataToCacheFromTBE, "mt", desc="Write data from TBE to cache (for WB stores)") {
        assert(L2_TBEs[address].Dirty == true);
        getL2CacheEntry(address).DataBlk := L2_TBEs[address].DataBlk;
        getL2CacheEntry(address).Dirty := L2_TBEs[address].Dirty;
    }

    action(ss_recordGetSL1ID, "\s", desc="Record L1 GetS for load response") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            assert(L2_TBEs[address].Space == in_msg.Space);
            L2_TBEs[address].L1_GetS_IDs.add(in_msg.Requestor);
            L2_TBEs[address].RequestSize := getIntMax(L2_TBEs[address].RequestSize, in_msg.RequestSize);   // set size to max of all merged GetS
            L2_TBEs[address].mfset.insert(in_msg.Requestor, in_msg.memfetch);
        }
    }

    action(set_setMRU, "\set", desc="set the MRU entry") {
        L2cacheMemory.setMRU(address);
    }

    action(qq_allocateL2CacheBlock, "\q", desc="Set L2 cache tag equal to tag of block B.") {
        if (L2cacheMemory.isTagPresent(address) == false) {
            L2cacheMemory.allocate(address);
        }
    }

    action(t_updateL2TimeRead, "t", desc="Extend the lifetime of the L2 block according to read request") {
        peek(L1RequestIntraChipL2Network_in,  RequestMsg) {
            // Extend time to current time + delta requested if it is greater than the already set timestamp
            getL2CacheEntry(address).liveUntilTime := globalTimeAfterRead(in_msg.deltaRequested, getL2CacheEntry(address).liveUntilTime);
        }
    }

    action(tt_updateL2TimeRead_fromTBE, "tt", desc="Extend the lifetime of the L2 block according to read request in TBE") {
        assert(isL2CacheTagPresent(address));
        // Compare against any possible liveUntilTime in TBE, extend it if necessary
        getL2CacheEntry(address).liveUntilTime := globalTimeAfterRead(L2_TBEs[address].deltaRequested, L2_TBEs[address].liveUntilTime);
    }

    action(ti_updateL2TimeWrite, "ti", desc="Extend the global timestamp by request (for upgrades) or increment it by 1 (for writes)") {
        // Take the max of (now+requested, liveUntil+1).
        // Writes request 0 time, so they will increment. Upgrades may extend time.
        peek(L1RequestIntraChipL2Network_in,  RequestMsg) {
            getL2CacheEntry(address).liveUntilTime := globalTimeAfterWrite(in_msg.deltaRequested,getL2CacheEntry(address).liveUntilTime);
        }
    }

    action(tn_expireL2BlockTime, "tn", desc="Set the L2 time to pre-evicted one (if it exists), else set time to now (expire it)") {
        // Compare against any possible liveUntilTime in TBE, otherwise set it to now
        getL2CacheEntry(address).liveUntilTime := globalTimeAfterWrite(int_to_time(0), L2_TBEs[address].liveUntilTime);
    }

    action(d_markBlockDirty, "d", desc="Mark block as dirty") {
        getL2CacheEntry(address).Dirty := true;
    }

    action(rr_deallocateL2CacheBlock, "\r", desc="Deallocate L2 cache block.  Sets the cache to not present, allowing a replacement in parallel with a fetch.") {
        L2cacheMemory.deallocate(address);
    }

    action(xx_recordGetXL1ID, "\x", desc="Record L1 WB for store response") {
        DEBUG_EXPR(address);
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            L2_TBEs[address].L1_WBAck_ID := in_msg.Requestor;
        }
        DEBUG_EXPR(address);
    }

    action(zz_recycleL1RequestQueue, "zz", desc="recycle L1 request queue") {
        L1RequestIntraChipL2Network_in.recycle();
    }

    action(zc_recycleL1ResponseNetwork, "zc", desc="recycle L1 response queue") {
       L1RequestIntraChipL2Network_in.recycle();
    }

    action(z_stall, "z", desc="stall - i.e. do nothing") {
    }


    //
    // Actions for profiling hit/miss rate
    //

    action(pM_profileRequestMiss, "prM", desc="Profile a demand miss for request message") {
       peek(L1RequestIntraChipL2Network_in, RequestMsg) {
         profile_L2Cache_request_g(convertRequestToGenericType(in_msg.Type), in_msg.MessageSize, id, true);   // miss
      }
    }

    action(pH_profileRequestHit, "prH", desc="Profile a demand hit for request message") {
       peek(L1RequestIntraChipL2Network_in, RequestMsg) {
         profile_L2Cache_request_g(convertRequestToGenericType(in_msg.Type), in_msg.MessageSize, id, false);  // hit
      }
    }


    action(pM_profileResponseMiss, "pwM", desc="Profile a demand miss for response message") {
       peek(L1RequestIntraChipL2Network_in, RequestMsg) {
         profile_L2Cache_request_g(convertRequestToGenericType(in_msg.Type), in_msg.MessageSize, id, true);   // miss
      }
    }

    action(pH_profileResponseHit, "pwH", desc="Profile a demand hit for response message") {
       peek(L1RequestIntraChipL2Network_in, RequestMsg) {
         profile_L2Cache_request_g(convertRequestToGenericType(in_msg.Type), in_msg.MessageSize, id, false);  // hit
      }
    }


    //
    // Generic actions for predictors
    //

    action(pdV_l2evict, "pdV", desc="Modify predictor lease due to L2 eviction of unexpired block") {
        if(is_pred_fixed()) {
            // Do nothing
        } else if(is_pred_global()) {
            pred_global_decrease_lease(get_pred_global_l2evict(), 0, 0);
        } else if(is_pred_perL2()) {
            pred_perL2_decrease_lease(version, get_pred_perL2_l2evict(), 0, 0);
        } else if(is_pred_pcadaptive()) {
            assert(L2cacheMemory.isTagPresent(address) == true);
            pred_pcadaptive_l2evict(getL2CacheEntry(address).SharerPCPairSet);
        } else if(is_pred_PCfixed()) {
            // Do nothing
        } else if(is_pred_Addrfixed()) {
            // Do nothing
        } else if(is_pred_oracle()) {
            ideal_inv_sharer_L1s(id, address, pred_oracle_get_sharers(getL2CacheEntry(address).SharerPCPairSet));
            assert(L2cacheMemory.isTagPresent(address) == true);
            getL2CacheEntry(address).liveUntilTime := get_time();
        } else if(is_pred_pcsampler()) {
            pred_pcsampler_endlifetime(get_time(), RealAddressToInt(address));
        } else {
            error("Invalid predictor type");
        }
    }

    action(pdEV_l2evict, "pdEV", desc="Modify predictor lease due to L2 eviction of expired block") {
        if(is_pred_fixed()) {
            // Do nothing
        } else if(is_pred_global()) {
            // Do nothing
        } else if(is_pred_perL2()) {
            // Do nothing
        } else if(is_pred_pcadaptive()) {
            // Do nothing
        } else if(is_pred_PCfixed()) {
            // Do nothing
        } else if(is_pred_Addrfixed()) {
            // Do nothing
        } else if(is_pred_oracle()) {
            // Do nothing
        } else if(is_pred_pcsampler()) {
            pred_pcsampler_endlifetime(get_time(), RealAddressToInt(address));
        } else {
            error("Invalid predictor type");
        }
    }

    action(pdL_load, "pdL", desc="Modify predictor lease due to a load at L2") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            if(is_pred_fixed()) {
                // Do nothing
            } else if(is_pred_global()) {
                // If block was expired in L1, increase time
                if(in_msg.ExpiredInL1 == true) {
                    pred_global_increase_lease(get_pred_global_expirehit(), 0, 0);
                }
            } else if(is_pred_perL2()) {
                // If block was expired in L1, increase time
                if(in_msg.ExpiredInL1 == true) {
                    pred_perL2_increase_lease(version, get_pred_perL2_expirehit(), 0, 0);
                }
            } else if(is_pred_pcadaptive()) {
                // Do nothing
            } else if(is_pred_PCfixed()) {
                // Do nothing
            } else if(is_pred_Addrfixed()) {
                // Do nothing
            } else if(is_pred_oracle()) {
                // Do nothing
            } else if(is_pred_pcsampler()) {
                pred_pcsampler_addloadpc(get_time(), RealAddressToInt(address), RealAddressToInt(in_msg.ProgramCounter));
            } else {
                error("Invalid predictor type");
            }
        }
    }

    action(pdEL_expiredLoad, "pdEL", desc="Modify predictor lease due to load hit at expired block") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            if(is_pred_fixed()) {
                // Do nothing
            } else if(is_pred_global()) {
                pred_global_increase_lease(get_pred_global_expirehit(), 0, 0);
            } else if(is_pred_perL2()) {
                pred_perL2_increase_lease(version, get_pred_perL2_expirehit(), 0, 0);
            } else if(is_pred_pcadaptive()) {
                assert(L2cacheMemory.isTagPresent(address) == true);
                pred_pcadaptive_expiredhit(getL2CacheEntry(address).SharerPCPairSet, getIntPair( RealAddressToInt(in_msg.ProgramCounter), IDToInt(machineIDToVersion(in_msg.Requestor)) ));
            } else if(is_pred_PCfixed()) {
                // Do nothing
            } else if(is_pred_Addrfixed()) {
                // Do nothing
            } else if(is_pred_oracle()) {
                // Do nothing
            } else if(is_pred_pcsampler()) {
                pred_pcsampler_addloadpc(get_time(), RealAddressToInt(address), RealAddressToInt(in_msg.ProgramCounter));
                pred_pcsampler_expiredloadhit(get_time(), RealAddressToInt(address), RealAddressToInt(in_msg.ProgramCounter));
            } else {
                error("Invalid predictor type");
            }
        }
    }

    action(pdW_unexpiredWrite, "pdW", desc="Modify predictor lease due to write to unexpired block") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            if(is_pred_fixed()) {
                // Do nothing
            } else if(is_pred_global()) {
                if(benchmark_contains_membar()) {
                    // Decrease only if timestamp is above fixed lease
                    pred_global_decrease_lease(get_pred_global_writeunexpired(), time_to_int(get_lease_fixed()), 0);
                }
            } else if(is_pred_perL2()) {
                if(benchmark_contains_membar()) {
                    // Decrease only if timestamp is above fixed lease
                    pred_perL2_decrease_lease(version, get_pred_perL2_writeunexpired(), time_to_int(get_lease_fixed()), 0);
                }
            } else if(is_pred_pcadaptive()) {
                if(benchmark_contains_membar()) {
                    assert(L2cacheMemory.isTagPresent(address) == true);
                    pred_pcadaptive_unexpiredwrite(getL2CacheEntry(address).SharerPCPairSet, getIntPair( RealAddressToInt(in_msg.ProgramCounter), IDToInt(machineIDToVersion(in_msg.Requestor)) ));
                }
            } else if(is_pred_PCfixed()) {
                // Do nothing
            } else if(is_pred_Addrfixed()) {
                // Do nothing
            } else if(is_pred_oracle()) {
                //reset timestamps of all sharers, plus the global timestamp (only if benchmark contains fences)
                if(benchmark_contains_membar()) {
                    ideal_inv_sharer_L1s(id, in_msg.Address, pred_oracle_get_sharers(getL2CacheEntry(address).SharerPCPairSet));
                    assert(L2cacheMemory.isTagPresent(address) == true);
                    getL2CacheEntry(address).liveUntilTime := get_time();
                }
            } else if(is_pred_pcsampler()) {
                if(benchmark_contains_membar()) {
                    pred_pcsampler_endlifetime(get_time(), RealAddressToInt(address));
                }
            } else {
                error("Invalid predictor type");
            }
        }
    }

    action(pdEW_expiredWrite, "pdEW", desc="Modify predictor lease due to write to expired block") {
            peek(L1RequestIntraChipL2Network_in, RequestMsg) {
                if(is_pred_fixed()) {
                    // Do nothing
                } else if(is_pred_global()) {
                    // Do nothing
                } else if(is_pred_perL2()) {
                    // Do nothing
                } else if(is_pred_pcadaptive()) {
                    // Do nothing
                } else if(is_pred_PCfixed()) {
                    // Do nothing
                } else if(is_pred_Addrfixed()) {
                    // Do nothing
                } else if(is_pred_oracle()) {
                    // Do nothing
                } else if(is_pred_pcsampler()) {
                    if(benchmark_contains_membar()) {
                        pred_pcsampler_endlifetime(get_time(), RealAddressToInt(address));
                    }
                } else {
                    error("Invalid predictor type");
                }
            }
        }

    action(pdA_unexpiredatomic, "pdA", desc="Modify predictor lease due to atomic to unexpired block") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            if(is_pred_fixed()) {
                // Do nothing
            } else if(is_pred_global()) {
                // Do nothing
            } else if(is_pred_perL2()) {
                // Do nothing
            } else if(is_pred_pcadaptive()) {
                assert(L2cacheMemory.isTagPresent(address) == true);
                pred_pcadaptive_unexpiredatomic(getL2CacheEntry(address).SharerPCPairSet, getIntPair( RealAddressToInt(in_msg.ProgramCounter), IDToInt(machineIDToVersion(in_msg.Requestor)) ));
            } else if(is_pred_PCfixed()) {
                // Do nothing
            } else if(is_pred_Addrfixed()) {
                // Do nothing
            } else if(is_pred_oracle()) {
                //reset timestamps of all sharers, plus the global timestamp (only if benchmark contains fences)
                if(benchmark_contains_membar()) {
                    ideal_inv_sharer_L1s(id, in_msg.Address, pred_oracle_get_sharers(getL2CacheEntry(address).SharerPCPairSet));
                    assert(L2cacheMemory.isTagPresent(address) == true);
                    getL2CacheEntry(address).liveUntilTime := get_time();
                }
            } else if(is_pred_pcsampler()) {
                if(benchmark_contains_membar()) {
                    pred_pcsampler_endlifetime(get_time(), RealAddressToInt(address));
                }
            } else {
                error("Invalid predictor type");
            }
        }
    }

    action(pdEA_expiredatomic, "pdEA", desc="Modify predictor lease due to atomic to expired block") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            if(is_pred_fixed()) {
                // Do nothing
            } else if(is_pred_global()) {
                // Do nothing
            } else if(is_pred_perL2()) {
                // Do nothing
            } else if(is_pred_pcadaptive()) {
                // Do nothing
            } else if(is_pred_PCfixed()) {
                // Do nothing
            } else if(is_pred_Addrfixed()) {
                // Do nothing
            } else if(is_pred_oracle()) {
                // Do nothing
            } else if(is_pred_pcsampler()) {
                if(benchmark_contains_membar()) {
                    pred_pcsampler_endlifetime(get_time(), RealAddressToInt(address));
                }
            } else {
                error("Invalid predictor type");
            }
        }
    }

    //
    // Actions specifically for PCAdaptive predictor
    //
    action(pcC_pcClearAll, "pcC", desc="Clear sharer pc list") {
        assert(L2cacheMemory.isTagPresent(address) == true);
        getL2CacheEntry(address).SharerPCPairSet.clear();
    }
    action(pcA_pcAdd, "pcA", desc="Add to sharer pc list") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            assert(L2cacheMemory.isTagPresent(address) == true);
            getL2CacheEntry(address).SharerPCPairSet.insert(getIntPair( RealAddressToInt(in_msg.ProgramCounter), IDToInt(machineIDToVersion(in_msg.Requestor)) ));
        }
    }
    action(pcX_pcExclusive, "pcX", desc="Remove all sharers except writing sharer if it exists") {
        peek(L1RequestIntraChipL2Network_in, RequestMsg) {
            assert(L2cacheMemory.isTagPresent(address) == true);
            getL2CacheEntry(address).SharerPCPairSet := removeMatchingSecondElement( getL2CacheEntry(address).SharerPCPairSet, IDToInt(machineIDToVersion(in_msg.Requestor)) );
        }
    }

    //
    // Actions for lifetime tracers
    //
    action(ltL_lifetimeTraceLease, "ltL", desc="Record lease in lifetime tracer") {
       if(lifetimeTrace_enabled()) {
           peek(L1RequestIntraChipL2Network_in,  RequestMsg) {
               lifetimeTrace_lease(get_time(), RealAddressToInt(address), machineIDToVersion(in_msg.Requestor), RealAddressToInt(in_msg.ProgramCounter), in_msg.deltaRequested);
           }
       }
    }


    //*****************************************************
    // TRANSITIONS
    //*****************************************************


    //===============================================
    // BASE STATE - I

    // Transitions from I (Idle)
    transition(NP, L1_GETS,  IS) {
        pdL_load;
        pM_profileRequestMiss;
        ltL_lifetimeTraceLease;
        qq_allocateL2CacheBlock;
        pcC_pcClearAll;
        pcA_pcAdd;
        i_allocateTBE;
        ss_recordGetSL1ID;
        a_issueFetchToMemory;
        jj_popL1RequestQueue;
    }

    // Write in Idle state - transition to IM and send back an ACK after IM completes
    // Upgrade in Idle state, likely here because Upgrade got here after block replaced.
    // Transition to IM and send back an Ack, L1 will know this block has expired and will be expecting an Ack
    transition(NP, {L1_Write, L1_Upgrade_NT},  IM) {
        pM_profileResponseMiss;
        qq_allocateL2CacheBlock;
        pcC_pcClearAll;
        d_markBlockDirty;
        il_allocateTBE;
        xx_recordGetXL1ID;
        a_issueFetchToMemoryForStore;
        jj_popL1RequestQueue;
    }

    // Atomic in Idle state - transition to IMA and send back an Atomic data after IMA completes
    transition(NP, L2_Atomic,  IMA) {
        pM_profileResponseMiss;
        qq_allocateL2CacheBlock;
        pcC_pcClearAll;
        d_markBlockDirty;
        il_allocateTBE;
        xx_recordGetXL1ID;
        a_issueFetchToMemoryForStore;
        jj_popL1RequestQueue;
    }



    // Replacement to transient state
    transition({IS, IM, IMA}, {L2_Replacement, L2_Replacement_clean}) {
        // Do nothing
        // We can't recycle queue because his event can be triggered from both request and response queue
        z_stall;
    }

    // transitions from IS
    transition(IS, L1_GETS) {
        pdL_load;
        pH_profileRequestHit;
        ltL_lifetimeTraceLease;
        pcA_pcAdd;
        ss_recordGetSL1ID;
        jj_popL1RequestQueue;
    }

    // Got data from memory with a single sharer waiting
    transition(IS, Mem_Data, S) {
        m_writeDataToCache;
        tt_updateL2TimeRead_fromTBE;
        dst_sendDataToGetSRequestors;
        s_deallocateTBE;
        o_popIncomingResponseQueue;
    }

    // Got data from memory with multiple sharers waiting
    transition(IS, Mem_Data_SS, SS) {
        m_writeDataToCache;
        tt_updateL2TimeRead_fromTBE;
        dst_sendDataToGetSRequestors;
        s_deallocateTBE;
        o_popIncomingResponseQueue;
    }

    // Servicing a write that missed in the L2, send ACK and go to Expired state
    transition(IM, Mem_Data, S) {
        m_writeDataToCache;
        tn_expireL2BlockTime;
        mt_writeDataToCacheFromTBE;
        wagt_sendAckGlobalToRequestorFromTBE;
        s_deallocateTBE;
        o_popIncomingResponseQueue;
    }

    // Servicing an Atomic that missed in the L2, send Atomic data and go to Expired state
    transition(IMA, Mem_Data, S) {
        m_writeDataToCache;
        tn_expireL2BlockTime;
        mt_writeDataToCacheFromTBE;
        dst_sendAtomicDataToGetSRequestors;
        s_deallocateTBE;
        o_popIncomingResponseQueue;
    }

    // Load request in a transient state, recycle the load queue
    transition({IM, IMA}, {L1_GETS}) {
        //zz_recycleL1RequestQueue;
        z_stall;
    }

    // transitions from S
    transition(S, L1_GETS, SS) {
        pdL_load;
        pH_profileRequestHit;
        ltL_lifetimeTraceLease;
        pcA_pcAdd;
        t_updateL2TimeRead;
        ds_sendDataToRequestor;
        set_setMRU;
        jj_popL1RequestQueue;
    }

    // Received a Write, send back an Ack Global
    // Transition to S state
    transition({S, SS}, L1_Write, S) {
        pH_profileResponseHit;
        pdW_unexpiredWrite;
        pcX_pcExclusive;
        d_markBlockDirty;
        wa_g_sendAckGlobalToRequestor;
        ti_updateL2TimeWrite;
        mr_writeDataToCacheFromRequest;
        set_setMRU;
        jj_popL1RequestQueue;
    }

    // Received a Write in expired state, send back an Ack (non-global)
    transition(E, L1_Write, S) {
        pdEW_expiredWrite;
        pH_profileResponseHit;
        pcC_pcClearAll;
        d_markBlockDirty;
        wa_sendAckToRequestor;
        ti_updateL2TimeWrite;
        mr_writeDataToCacheFromRequest;
        set_setMRU;
        jj_popL1RequestQueue;
    }

    // Got an Upgrade with matching timestamp, send back an Ack (non-global)
    transition(S, L1_Upgrade_T) {
        pH_profileResponseHit;
        d_markBlockDirty;
        wa_sendAckToRequestor;
        ti_updateL2TimeWrite;
        mr_writeDataToCacheFromRequest;
        set_setMRU;
        jj_popL1RequestQueue;
    }

    // Got an upgrade and timestamps don't match - need to send back data_global
    transition({S,SS}, L1_Upgrade_NT, S) {
        pH_profileResponseHit;
        pdW_unexpiredWrite;
        pcX_pcExclusive;
        d_markBlockDirty;
        ds_g_sendDataGlobalToRequestor;
        ti_updateL2TimeWrite;
        mr_writeDataToCacheFromRequest;
        set_setMRU;
        jj_popL1RequestQueue;
    }

    // Got an Atomic request to valid states, send data_atomic back
    // In case of atomic requests, we never check for time differences and always send back a GLOBAL message
    // (L1 treats DATA_ATOMIC as GLOBAL and always updates global-write timestamp)
    transition({S,SS}, L2_Atomic, S) {
        pdA_unexpiredatomic;
        pH_profileResponseHit;
        pcC_pcClearAll;
        d_markBlockDirty;
        ds_sendAtomicDataGlobalToRequestor;
        ti_updateL2TimeWrite;
        mr_writeDataToCacheFromRequest;
        set_setMRU;
        jj_popL1RequestQueue;
    }

    // Clean replacement to unexpired timestamp, need to wait for expire event
    transition({S,SS}, {L2_Replacement_clean}, MI) {
        pdV_l2evict;
        ir_allocateTBE;
        it_insertL2ExpireTriggerQueue;
        rr_deallocateL2CacheBlock;
    }

    // Dirty replacement and unexpired timestamp, do writeback to DRAM and wait for expire event
    transition({S,SS}, {L2_Replacement}, MI) {
        pdV_l2evict;
        ir_allocateTBE;
        it_insertL2ExpireTriggerQueue;
        c_exclusiveReplacement;
        rr_deallocateL2CacheBlock;
    }

    // transitions from SS
    transition(SS, {L1_GETS}) {
        pdL_load;
        pH_profileRequestHit;
        ltL_lifetimeTraceLease;
        pcA_pcAdd;
        t_updateL2TimeRead;
        ds_sendDataToRequestor;
        set_setMRU;
        jj_popL1RequestQueue;
    }

    transition(SS, L1_Upgrade_T, S) {
        pH_profileResponseHit;
        pdW_unexpiredWrite;
        pcX_pcExclusive;
        d_markBlockDirty;
        wa_g_sendAckGlobalToRequestor;
        ti_updateL2TimeWrite;
        mr_writeDataToCacheFromRequest;
        set_setMRU;
        jj_popL1RequestQueue;
    }

    // transitions from E
    transition(E, L1_GETS, S) {
        pdEL_expiredLoad;
        ltL_lifetimeTraceLease;
        pcA_pcAdd;
        pH_profileRequestHit;
        t_updateL2TimeRead;
        ds_sendDataToRequestor;
        set_setMRU;
        jj_popL1RequestQueue;
    }

    // Got an upgrade in expired state, could happen if block expired after L1 sent it but before message got here
    // L1 will know that this block has expired, so send an Ack back (non-global because everyone else is expired)
    transition(E, {L1_Upgrade_NT}, S) {
        pdEW_expiredWrite;
        pH_profileResponseHit;
        pcC_pcClearAll;
        d_markBlockDirty;
        wa_sendAckToRequestor;
        ti_updateL2TimeWrite;
        mr_writeDataToCacheFromRequest;
        set_setMRU;
        jj_popL1RequestQueue;
    }

    // Got an Atomic in expired state, send back Atomic data
    // Got an upgrade in expired state, could happen if block expired after L1 sent it but before message got here
    // L1 will know that this block has expired, so send an Ack back (non-global because everyone else is expired)
    transition(E, L2_Atomic, S) {
       pdEA_expiredatomic;
       pH_profileResponseHit;
       pcC_pcClearAll;
       d_markBlockDirty;
       ds_sendAtomicDataGlobalToRequestor;
       ti_updateL2TimeWrite;
       mr_writeDataToCacheFromRequest;
       set_setMRU;
       jj_popL1RequestQueue;
    }

    // Clean L2 replacement to expired timestamp, go straight to NP
    transition(E, {L2_Replacement_clean}, NP) {
        pdEV_l2evict;
        rr_deallocateL2CacheBlock;
    }

    // Dirty L2 replacement expired state, do writeback to DRAM and go to NP
    transition(E, {L2_Replacement}, NP) {
        pdEV_l2evict;
        c_exclusiveReplacement;
        rr_deallocateL2CacheBlock;
    }


    // Transitions from MI
    transition(MI, L1_GETS,  IS) {
        pdL_load;
        pM_profileRequestMiss;
        ltL_lifetimeTraceLease;
        qq_allocateL2CacheBlock;
        pcC_pcClearAll;
        pcA_pcAdd;
        is_setTBE;
        ss_recordGetSL1ID;
        a_issueFetchToMemory;
        jj_popL1RequestQueue;
    }

    // Write in MI state - transition to IM and send back an ACK after IM completes
    // Upgrade in Idle state, likely here because Upgrade got here after block replaced.
    // Transition to IM and send back an Ack, L1 will know this block has expired and will be expecting an Ack
    transition(MI, {L1_Write, L1_Upgrade_NT},  IM) {
        pM_profileResponseMiss;
        qq_allocateL2CacheBlock;
        pcC_pcClearAll;
        d_markBlockDirty;
        ils_setTBE;
        xx_recordGetXL1ID;
        a_issueFetchToMemoryForStore;
        jj_popL1RequestQueue;
    }

    // Atomic in MI state - transition to IMA and send back an Atomic data after IMA completes
    transition(MI, L2_Atomic,  IMA) {
        pM_profileResponseMiss;
        qq_allocateL2CacheBlock;
        pcC_pcClearAll;
        d_markBlockDirty;
        ils_setTBE;
        xx_recordGetXL1ID;
        a_issueFetchToMemoryForStore;
        jj_popL1RequestQueue;
    }



    // Expire event received for evicted block, can go to NP now
    transition(MI, L2_Expire, NP) {
        s_deallocateTBE;
        ot_popL2ExpireTriggerQueue;
    }


    // Common transitions
    transition({IS, IM, IMA}, {L1_Write, L1_Upgrade_NT, L2_Atomic}) {
        //zc_recycleL1ResponseNetwork;
        z_stall;
    }


}

